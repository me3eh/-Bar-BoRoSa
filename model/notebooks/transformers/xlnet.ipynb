{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"XTOKeeBnI-V0\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Fake News Detection With Transformer Models XLNET\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 48,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"bPtajH4yI-V2\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# importing required libaries\\n\",\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import pandas as pd\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"import seaborn as sns\\n\",\n",
    "        \"from sklearn.model_selection import train_test_split\\n\",\n",
    "        \"from sklearn.metrics import accuracy_score\\n\",\n",
    "        \"from sklearn.metrics import classification_report\\n\",\n",
    "        \"from sklearn.metrics import confusion_matrix\\n\",\n",
    "        \"import pickle\\n\",\n",
    "        \"from transformers import XLNetConfig, XLNetModel\\n\",\n",
    "        \"import torch\\n\",\n",
    "        \"from transformers import XLNetTokenizer, XLNetForSequenceClassification\\n\",\n",
    "        \"from transformers import AdamW\\n\",\n",
    "        \"from torch.utils.data import DataLoader, Dataset\\n\",\n",
    "        \"import re\\n\",\n",
    "        \"import pickle\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"import os\\n\",\n",
    "        \"from google.colab import drive\\n\",\n",
    "        \"MOUNTPOINT = '/content/gdrive'\\n\",\n",
    "        \"DATADIR = os.path.join(MOUNTPOINT, 'My Drive', 'myfolder')\\n\",\n",
    "        \"drive.mount(MOUNTPOINT)\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"3WMD6YsGjQBs\",\n",
    "        \"outputId\": \"7ccd0bdd-497c-4c6c-c0ea-9fdad28817f3\"\n",
    "      },\n",
    "      \"execution_count\": 30,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\\\"/content/gdrive\\\", force_remount=True).\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"YWtUhns5jd8f\"\n",
    "      },\n",
    "      \"execution_count\": 10,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 49,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"DPhigfd6I-V3\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"\\n\",\n",
    "        \"# Initializing a XLNet configuration\\n\",\n",
    "        \"configuration = XLNetConfig()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Initializing a model (with random weights) from the configuration\\n\",\n",
    "        \"model = XLNetModel(configuration)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Accessing the model configuration\\n\",\n",
    "        \"configuration = model.config\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 50,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"LU8LGCOYI-V4\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"class FakeNewsDataset(Dataset):\\n\",\n",
    "        \"    def __init__(self, texts, labels, tokenizer, max_length):\\n\",\n",
    "        \"        self.texts = texts\\n\",\n",
    "        \"        self.labels = labels\\n\",\n",
    "        \"        self.tokenizer = tokenizer\\n\",\n",
    "        \"        self.max_length = max_length\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def __len__(self):\\n\",\n",
    "        \"        return len(self.texts)\\n\",\n",
    "        \"\\n\",\n",
    "        \"    def __getitem__(self, idx):\\n\",\n",
    "        \"       text = str(self.texts[idx])\\n\",\n",
    "        \"       label = self.labels[idx]\\n\",\n",
    "        \"\\n\",\n",
    "        \"       # tokenize the text\\n\",\n",
    "        \"       encoding = self.tokenizer.encode_plus(\\n\",\n",
    "        \"           text,\\n\",\n",
    "        \"           add_special_tokens=True,\\n\",\n",
    "        \"           max_length=self.max_length,\\n\",\n",
    "        \"           return_token_type_ids=False,\\n\",\n",
    "        \"           padding='max_length',\\n\",\n",
    "        \"           truncation=True,\\n\",\n",
    "        \"           return_tensors='pt'\\n\",\n",
    "        \"       )\\n\",\n",
    "        \"\\n\",\n",
    "        \"       # get the input ids\\n\",\n",
    "        \"       input_ids = encoding['input_ids'].flatten()\\n\",\n",
    "        \"\\n\",\n",
    "        \"       # create the attention mask\\n\",\n",
    "        \"       attention_mask = torch.ones_like(input_ids)\\n\",\n",
    "        \"\\n\",\n",
    "        \"       # if 'attention_mask' exists in encoding, use it\\n\",\n",
    "        \"       if 'attention_mask' in encoding:\\n\",\n",
    "        \"           attention_mask = encoding['attention_mask'].flatten()\\n\",\n",
    "        \"           return {\\n\",\n",
    "        \"           'input_ids': input_ids,\\n\",\n",
    "        \"           'attention_mask': attention_mask,\\n\",\n",
    "        \"           'label': torch.tensor(label, dtype=torch.long)\\n\",\n",
    "        \"       }\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 33,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"V05gIxy_I-V4\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def train(model, train_loader, optimizer, criterion, device):\\n\",\n",
    "        \"    model.train()\\n\",\n",
    "        \"    total_loss = 0.0\\n\",\n",
    "        \"    for batch in train_loader:\\n\",\n",
    "        \"        input_ids = batch['input_ids'].to(device)\\n\",\n",
    "        \"        attention_mask = batch['attention_mask'].to(device)\\n\",\n",
    "        \"        labels = batch['label'].to(device)\\n\",\n",
    "        \"\\n\",\n",
    "        \"        optimizer.zero_grad()\\n\",\n",
    "        \"        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\\n\",\n",
    "        \"        loss = outputs.loss\\n\",\n",
    "        \"        total_loss += loss.item()\\n\",\n",
    "        \"\\n\",\n",
    "        \"        loss.backward()\\n\",\n",
    "        \"        optimizer.step()\\n\",\n",
    "        \"\\n\",\n",
    "        \"    return total_loss / len(train_loader)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Example function for evaluating the model\\n\",\n",
    "        \"def evaluate(model, eval_loader, criterion, device):\\n\",\n",
    "        \"    model.eval()\\n\",\n",
    "        \"    total_loss = 0.0\\n\",\n",
    "        \"    correct = 0\\n\",\n",
    "        \"    total = 0\\n\",\n",
    "        \"    with torch.no_grad():\\n\",\n",
    "        \"        for batch in eval_loader:\\n\",\n",
    "        \"            input_ids = batch['input_ids'].to(device)\\n\",\n",
    "        \"            attention_mask = batch['attention_mask'].to(device)\\n\",\n",
    "        \"            labels = batch['label'].to(device)\\n\",\n",
    "        \"\\n\",\n",
    "        \"            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\\n\",\n",
    "        \"            loss = outputs.loss\\n\",\n",
    "        \"            total_loss += loss.item()\\n\",\n",
    "        \"\\n\",\n",
    "        \"            predictions = torch.argmax(outputs.logits, dim=1)\\n\",\n",
    "        \"            correct += (predictions == labels).sum().item()\\n\",\n",
    "        \"            total += labels.size(0)\\n\",\n",
    "        \"\\n\",\n",
    "        \"    accuracy = correct / total\\n\",\n",
    "        \"    return total_loss / len(eval_loader), accuracy\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 51,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"AuSeICdkI-V5\",\n",
    "        \"outputId\": \"d88eec34-6a93-43fa-be92-ce8bfb1d6467\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stderr\",\n",
    "          \"text\": [\n",
    "            \"Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\\n\",\n",
    "            \"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\\n\",\n",
    "        \"model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\\n\",\n",
    "        \"from sklearn.model_selection import train_test_split\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 35,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"P5345SelVr8e\",\n",
    "        \"outputId\": \"c2eaa4cf-0c8e-4735-91c4-4512b19f88a7\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"mkdir: cannot create directory ‘/root/.kaggle’: File exists\\n\",\n",
    "            \"Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\\n\",\n",
    "            \"fake-news-detection.zip: Skipping, found more recently modified local copy (use --force to force download)\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"! pip install -q kaggle\\n\",\n",
    "        \"! mkdir ~/.kaggle\\n\",\n",
    "        \"! touch ~/.kaggle/kaggle.json\\n\",\n",
    "        \"!echo '{\\\"username\\\":\\\"akselsaatci\\\",\\\"key\\\":\\\"asdasdasd\\\"}' > ~/.kaggle/kaggle.json\\n\",\n",
    "        \"! kaggle datasets download -d vishakhdapat/fake-news-detection\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 36,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"KRuKgNR1YBqi\",\n",
    "        \"outputId\": \"e4c5b3a6-8a51-4927-fdcb-7e5267730195\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Archive:  fake-news-detection.zip\\n\",\n",
    "            \"replace fake_and_real_news.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: \"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"! unzip fake-news-detection.zip\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 52,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"pzjnvrHZJzJK\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"data = pd.read_csv('./fake_and_real_news.csv')\\n\",\n",
    "        \"# handle duplicated values\\n\",\n",
    "        \"data.drop_duplicates(inplace=True)\\n\",\n",
    "        \"data.dropna(inplace=True)  # Remove rows with missing values\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"data['is_fake'] = 0\\n\",\n",
    "        \"\\n\",\n",
    "        \"data['is_fake'] = (data['label'] == 'Fake').astype(int)\\n\",\n",
    "        \"\\n\",\n",
    "        \"def preprocess(text):\\n\",\n",
    "        \"    text = re.sub(r'[^\\\\w\\\\s\\\\']', ' ', text)\\n\",\n",
    "        \"    text = re.sub(r' +', ' ', text)\\n\",\n",
    "        \"    return text.strip().lower()\\n\",\n",
    "        \"\\n\",\n",
    "        \"data['Text'] = data['Text'].apply(preprocess)\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"x_train, x_test, y_train, y_test = train_test_split(data['Text'], data['is_fake'], test_size=0.3, random_state=0)\\n\",\n",
    "        \"\\n\",\n",
    "        \"x_train.reset_index(drop=True, inplace=True)\\n\",\n",
    "        \"x_test.reset_index(drop=True, inplace=True)\\n\",\n",
    "        \"y_train.reset_index(drop=True, inplace=True)\\n\",\n",
    "        \"y_test.reset_index(drop=True, inplace=True)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"filtered_y_test = [y_test[i] for i in range(len(y_test)) if y_test[i] == 1]\\n\",\n",
    "        \"print(filtered_y_test)\\n\",\n",
    "        \"\\n\",\n",
    "        \"filtered_y_train = [y_train[i] for i in range(len(y_train)) if y_train[i] == 1]\\n\",\n",
    "        \"print(filtered_y_train)\\n\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"6JvUHRqwxDy6\",\n",
    "        \"outputId\": \"e20d8678-6337-4888-f141-5aed8e36632b\"\n",
    "      },\n",
    "      \"execution_count\": 55,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\\n\",\n",
    "            \"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 61,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"OaUJvqXfJ5Gz\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"BATCH_SIZE = 8\\n\",\n",
    "        \"MAX_LENGTH = 32\\n\",\n",
    "        \"LEARNING_RATE = 2e-5\\n\",\n",
    "        \"EPOCHS = 8\\n\",\n",
    "        \"\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 62,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"S7H8ts-fKNMG\",\n",
    "        \"outputId\": \"49966b7b-5560-4457-e9a7-91b8ff9ab70f\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Epoch 1/8:\\n\",\n",
    "            \"Training Loss: 0.0732\\n\",\n",
    "            \"Evaluation Loss: 0.0122 | Evaluation Accuracy: 0.9976\\n\",\n",
    "            \"Epoch 2/8:\\n\",\n",
    "            \"Training Loss: 0.0033\\n\",\n",
    "            \"Evaluation Loss: 0.0110 | Evaluation Accuracy: 0.9983\\n\",\n",
    "            \"Epoch 3/8:\\n\",\n",
    "            \"Training Loss: 0.0000\\n\",\n",
    "            \"Evaluation Loss: 0.0119 | Evaluation Accuracy: 0.9990\\n\",\n",
    "            \"Epoch 4/8:\\n\",\n",
    "            \"Training Loss: 0.0136\\n\",\n",
    "            \"Evaluation Loss: 0.0112 | Evaluation Accuracy: 0.9983\\n\",\n",
    "            \"Epoch 5/8:\\n\",\n",
    "            \"Training Loss: 0.0026\\n\",\n",
    "            \"Evaluation Loss: 0.0103 | Evaluation Accuracy: 0.9990\\n\",\n",
    "            \"Epoch 6/8:\\n\",\n",
    "            \"Training Loss: 0.0009\\n\",\n",
    "            \"Evaluation Loss: 0.0114 | Evaluation Accuracy: 0.9990\\n\",\n",
    "            \"Epoch 7/8:\\n\",\n",
    "            \"Training Loss: 0.0000\\n\",\n",
    "            \"Evaluation Loss: 0.0120 | Evaluation Accuracy: 0.9990\\n\",\n",
    "            \"Epoch 8/8:\\n\",\n",
    "            \"Training Loss: 0.0000\\n\",\n",
    "            \"Evaluation Loss: 0.0125 | Evaluation Accuracy: 0.9990\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"train_dataset = FakeNewsDataset(x_train, y_train, tokenizer, MAX_LENGTH)\\n\",\n",
    "        \"eval_dataset = FakeNewsDataset(x_test, y_test, tokenizer, MAX_LENGTH)\\n\",\n",
    "        \"\\n\",\n",
    "        \"    # Create data loaders\\n\",\n",
    "        \"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\\n\",\n",
    "        \"eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE)\\n\",\n",
    "        \"\\n\",\n",
    "        \"    # Move model to GPU if available\\n\",\n",
    "        \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "        \"model.to(device)\\n\",\n",
    "        \"\\n\",\n",
    "        \"    # Define loss function and optimizer\\n\",\n",
    "        \"criterion = torch.nn.CrossEntropyLoss()\\n\",\n",
    "        \"optimizer =  torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\\n\",\n",
    "        \"\\n\",\n",
    "        \"    # Training loop\\n\",\n",
    "        \"for epoch in range(EPOCHS):\\n\",\n",
    "        \"  train_loss = train(model, train_loader, optimizer, criterion, device)\\n\",\n",
    "        \"  eval_loss, eval_accuracy = evaluate(model, eval_loader, criterion, device)\\n\",\n",
    "        \"  print(f'Epoch {epoch+1}/{EPOCHS}:')\\n\",\n",
    "        \"  print(f'Training Loss: {train_loss:.4f}')\\n\",\n",
    "        \"  print(f'Evaluation Loss: {eval_loss:.4f} | Evaluation Accuracy: {eval_accuracy:.4f}')\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"with open(\\\"./gdrive/MyDrive/changed_with_eight_epocs_xlnet.pkl\\\", \\\"wb\\\") as file: # file is a variable for storing the newly created file, it can be anything.\\n\",\n",
    "        \"    pickle.dump(model, file) # Dump function is used to write the object into the created file in byte format.\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"ZI4eBSNRso30\"\n",
    "      },\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 77,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"dCcdxl3pLXx5\",\n",
    "        \"outputId\": \"68bc42a6-721c-481e-9b72-68ad55f2868d\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"execute_result\",\n",
    "          \"data\": {\n",
    "            \"text/plain\": [\n",
    "              \"XLNetForSequenceClassification(\\n\",\n",
    "              \"  (transformer): XLNetModel(\\n\",\n",
    "              \"    (word_embedding): Embedding(32000, 768)\\n\",\n",
    "              \"    (layer): ModuleList(\\n\",\n",
    "              \"      (0-11): 12 x XLNetLayer(\\n\",\n",
    "              \"        (rel_attn): XLNetRelativeAttention(\\n\",\n",
    "              \"          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n\",\n",
    "              \"          (dropout): Dropout(p=0.1, inplace=False)\\n\",\n",
    "              \"        )\\n\",\n",
    "              \"        (ff): XLNetFeedForward(\\n\",\n",
    "              \"          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n\",\n",
    "              \"          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\\n\",\n",
    "              \"          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\\n\",\n",
    "              \"          (dropout): Dropout(p=0.1, inplace=False)\\n\",\n",
    "              \"          (activation_function): GELUActivation()\\n\",\n",
    "              \"        )\\n\",\n",
    "              \"        (dropout): Dropout(p=0.1, inplace=False)\\n\",\n",
    "              \"      )\\n\",\n",
    "              \"    )\\n\",\n",
    "              \"    (dropout): Dropout(p=0.1, inplace=False)\\n\",\n",
    "              \"  )\\n\",\n",
    "              \"  (sequence_summary): SequenceSummary(\\n\",\n",
    "              \"    (summary): Linear(in_features=768, out_features=768, bias=True)\\n\",\n",
    "              \"    (activation): Tanh()\\n\",\n",
    "              \"    (first_dropout): Identity()\\n\",\n",
    "              \"    (last_dropout): Dropout(p=0.1, inplace=False)\\n\",\n",
    "              \"  )\\n\",\n",
    "              \"  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\\n\",\n",
    "              \")\"\n",
    "            ]\n",
    "          },\n",
    "          \"metadata\": {},\n",
    "          \"execution_count\": 77\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"with open('./gdrive/MyDrive/changed_with_eight_epocs_xlnet.pkl', 'rb') as f:\\n\",\n",
    "        \"    loaded_model = pickle.load(f)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Ensure to move the model to the appropriate device if necessary\\n\",\n",
    "        \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "        \"loaded_model.to(device)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 78,\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"4UnwwLWXK0pE\",\n",
    "        \"outputId\": \"7acbd3ab-a914-4b28-c9c5-548deb2dee02\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Predicted class: fake\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"\\n\",\n",
    "        \"# Load your XLNet model and tokenizer\\n\",\n",
    "        \"model = loaded_model\\n\",\n",
    "        \"\\n\",\n",
    "        \"input_text = \\\"Anthem to pare back Obamacare offerings in Nevada and Georgia (Reuters) - U.S. health insurer Anthem Inc (ANTM.N) said on Monday it will no longer offer Obamacare plans in Nevada’s state exchange and will stop offering the plans in nearly half of Georgia’s counties next year. The moves come after Republican senators last month failed to repeal and replace Obamacare, former President Barack Obama’s signature healthcare reform law, creating uncertainty over how the program providing health benefits to 20 million Americans will be funded and managed in 2018. Hundreds of U.S. counties are at risk of losing access to private health coverage in 2018 as insurers consider pulling out of those markets in the coming months. Nevada had said in June that residents in 14 counties out of 17 in the state would not have access to qualified health plans on the state exchanges. Anthem’s decision to leave the state entirely does not increase the number of “bare counties” in the state, Nevada Insurance Commissioner Barbara Richardson said in a statement. The insurer will still offer “catastrophic plans,” which can be purchased outside the state’s exchange and are only available to consumers under 30 years old or with a low income. Anthem also said it will only offer Obamacare plans in 85 of Georgia’s 159 counties. It said the counties it will continue to offer the plans in are mostly rural counties that would otherwise not have health insurance coverage for their residents.  It said these changes do not impact Anthem’s Medicare Advantage, Medicaid or employer-based plans in either state.   The company said last week that it will pull out of 16 of 19 pricing regions in California in 2018 where it offered Obamacare options this year.  Anthem blamed the moves in part on uncertainty over whether the Trump administration would maintain subsidies that keep costs down.  U.S. President Donald Trump last week threatened to cut off subsidy payments that make the plans affordable for lower-income Americans and help insurers to keep premiums down, after efforts to repeal the law signed by his predecessor, President Barack Obama, failed in Congress.  Trump has repeatedly urged Republican lawmakers to keep working to undo Obama’s Affordable Care Act.\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"inputs = tokenizer(input_text, return_tensors='pt', max_length=32 ,truncation=True)\\n\",\n",
    "        \"\\n\",\n",
    "        \"inputs.to(device)\\n\",\n",
    "        \"\\n\",\n",
    "        \"with torch.no_grad():\\n\",\n",
    "        \"    outputs = model(**inputs)\\n\",\n",
    "        \"\\n\",\n",
    "        \"predicted_label = torch.argmax(outputs.logits).item()\\n\",\n",
    "        \"\\n\",\n",
    "        \"label_map = {1: 'fake', 0: 'real'}\\n\",\n",
    "        \"predicted_class = label_map[predicted_label]\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Predicted class:\\\", predicted_class)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"KgmiSSGytsmI\"\n",
    "      },\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"accelerator\": \"GPU\",\n",
    "    \"colab\": {\n",
    "      \"gpuType\": \"T4\",\n",
    "      \"provenance\": []\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"display_name\": \"Python 3\",\n",
    "      \"name\": \"python3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"codemirror_mode\": {\n",
    "        \"name\": \"ipython\",\n",
    "        \"version\": 3\n",
    "      },\n",
    "      \"file_extension\": \".py\",\n",
    "      \"mimetype\": \"text/x-python\",\n",
    "      \"name\": \"python\",\n",
    "      \"nbconvert_exporter\": \"python\",\n",
    "      \"pygments_lexer\": \"ipython3\",\n",
    "      \"version\": \"3.11.8\"\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
