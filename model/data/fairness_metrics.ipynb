{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics on words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me3eh/Bar_BoRoSa/envs/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "# currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "# parentdir = os.path.dirname(currentdir)\n",
    "# sys.path.insert(0, parentdir)\n",
    "# import model\n",
    "from model.load_model import load_model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]2024/06/08 17:05:06 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "Downloading artifacts: 100%|██████████| 10/10 [00:00<00:00, 117.01it/s]\n",
      "/home/me3eh/Bar_BoRoSa/envs/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.5.0 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/me3eh/Bar_BoRoSa/envs/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 3793,  102]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "{'input_ids': tensor([[ 101, 3793,  102]]), 'attention_mask': tensor([[1, 1, 1]])}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DecisionTreeClassifier' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model, tokenizer, device \u001b[38;5;241m=\u001b[39m load_model()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bar_BoRoSa/model/load_model.py:38\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(text, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 38\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     predicted_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mlogits)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Map predicted label to class\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DecisionTreeClassifier' object is not callable"
     ]
    }
   ],
   "source": [
    "model, tokenizer, device = load_model()\n",
    "predict(\"text\", model, tokenizer, device)\n",
    "# predict(\"dupa\", model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "# importing the dataset\n",
    "first_data = pd.read_csv('../data/fake_and_real_data.csv')\n",
    "first_data.head()\n",
    "second_data = pd.read_csv('../data/WELFake_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "\n",
    "second_data.isnull().sum()\n",
    "\n",
    "# Dropping the missing values\n",
    "second_data = second_data.dropna()\n",
    "# Checking for duplicates\n",
    "\n",
    "second_data.duplicated().sum()\n",
    "\n",
    "# Drop unnamed column\n",
    "second_data.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinating title and text\n",
    "second_data['text'] = second_data['title'] + ' ' + second_data['text']\n",
    "# Dropping the title column\n",
    "second_data = second_data.drop(['title'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "first_data['label_t'] = 0\n",
    "\n",
    "\n",
    "first_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "first_data['label_t'] = (first_data['label'] != 'Fake').astype(int)\n",
    "first_data = first_data.drop(['label'], axis=1)\n",
    "first_data = first_data.rename(columns={'label_t': 'label'})\n",
    "first_data = first_data.rename(columns={'Text': 'text'})\n",
    "first_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unnecessary columns\n",
    "first_data.head()\n",
    "# converting the text to lowercase\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "first_data['text'] = first_data['text'].apply(preprocess)\n",
    "second_data['text'] = second_data['text'].apply(preprocess)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data.head()\n",
    "second_data.head()\n",
    "\n",
    "concat_data = pd.concat([first_data, second_data], axis=0)\n",
    "concat_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is trump inclouded \n",
    "\n",
    "concat_data['does_contain_trump'] = concat_data['text'].apply(lambda x: 'trump' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "# # model.predict(concat_data[\"text\"][None, :].numpy())\n",
    "# # import torch\n",
    "# # input_tensor = torch.tensor(concat_data.values, dtype=torch.float32)\n",
    "# vect = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "# xv_train = vect.fit_transform(concat_data[\"text\"])\n",
    "# model.predict(xv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data['does_contain_trump'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(concat_data[\"text\"])\n",
    "\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vect = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "# data = vect.transform(concat_data[\"text\"])\n",
    "# # xv_test = vect.transform(x_test)\n",
    "# model.predict(data)\n",
    "model.predict([[5]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_metrics(df):\n",
    "    \"\"\"Calculate fairness for subgroup of population\"\"\"\n",
    "    \n",
    "    #Confusion Matrix\n",
    "    cm=confusion_matrix(df['y'],df['y_pred'])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    N = TP+FP+FN+TN #Total population\n",
    "    ACC = (TP+TN)/N #Accuracy\n",
    "    TPR = TP/(TP+FN) # True positive rate\n",
    "    FPR = FP/(FP+TN) # False positive rate\n",
    "    FNR = FN/(TP+FN) # False negative rate\n",
    "    PPP = (TP + FP)/N # % predicted as positive\n",
    "    \n",
    "    return np.array([ACC, TPR, FPR, FNR, PPP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trump_unprivileged_group = [{'does_contain_trump': 0}] \n",
    "# trump_unprivileged_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness with all values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "# import os\n",
    "\n",
    "# # Define the run ID and artifact path\n",
    "# run_id = os.environ.get('RUN_ID')\n",
    "# # artifact_path = mlflow.get_artifact_uri(run_id)\n",
    "\n",
    "# # Print the artifact path to locate the MLmodel file\n",
    "# # print(\"Artifact path:\", artifact_path)\n",
    "# # loaded_model = mlflow.pytorch.load_model(artifact_path)\n",
    "# model_path = mlflow.artifacts.download_artifacts(run_id=run_id)\n",
    "# print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data[\"y\"] = [1 if y == 'Real' else 0 for y in concat_data['label']] \n",
    "concat_data[\"y_pred\"] = [model.predict(y) for y in concat_data['text']]\n",
    "result = fairness_metrics(concat_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness when having trump "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_containing_trump = concat_data.loc[concat_data['does_contain_trump'] == True]\n",
    "result = fairness_metrics(df_containing_trump)\n",
    "result\n",
    "# df_containing_trump\n",
    "# df[\"does_contain_trump\"]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_trump = concat_data.loc[concat_data['does_contain_trump'] == False]\n",
    "result = fairness_metrics(df_without_trump)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simply import it from sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dataset = concat_data \n",
    "\n",
    "# # Separating features from target feature\n",
    "# features = dataset.columns.tolist()\n",
    "# features.remove('label')\n",
    "# target = 'label'\n",
    "# X = dataset[features]\n",
    "# y = dataset[target]\n",
    "\n",
    "# # Define four sets and apply the function\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                     test_size=0.2, # 0.2 indicates a test set size of 20%\n",
    "#                                                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## applying fairness tests\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "# # Import the classifier and the metrics from sklearn\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "# dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "# vect = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "# xv_train = vect.fit_transform(X_train['text'])\n",
    "# xv_test = vect.transform(X_test['text'])\n",
    "\n",
    "\n",
    "# print(f\"Value to test if testing started\")\n",
    "# # The fit function will do the trick\n",
    "# dt_clf.fit(xv_train, y_train)\n",
    "\n",
    "# print(f\"Value to test if testing ended\")\n",
    "# # After the training phase, the model will be tested by predicting the values on the test set\n",
    "# dt_predictions = dt_clf.predict(xv_test)\n",
    "\n",
    "# dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "# dt_recall = recall_score(y_test, dt_predictions)\n",
    "# dt_f1_score = f1_score(y_test, dt_predictions)\n",
    "# print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n",
    "# print(f\"Decision Tree Recall: {dt_recall}\")\n",
    "# print(f\"Decision Tree F1 Score: {dt_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from aif360.datasets import BinaryLabelDataset\n",
    "# from aif360.datasets import StandardDataset\n",
    "# from aif360.metrics import ClassificationMetric\n",
    "\n",
    "\n",
    "# # We want to check the fairness level regarding the protected attribute \"sex\"\n",
    "# trump_features = ['does_contain_trump']\n",
    "\n",
    "\n",
    "# dataset = xv_test.toarray()\n",
    "# dataset['label'] = y_test  # and join the target feature with the others\n",
    "\n",
    "# predictions = dataset.copy(deep=True)  # we do the same task\n",
    "# # but this time the target feature is made by the predictions of our model\n",
    "# predictions['label'] = dt_predictions\n",
    "\n",
    "# # In this way, we have two datasets. One (dataset) is the original test set containing the original values of features,\n",
    "# # the other (predictions) contains the original values except for the target one, that is now made of model's predictions\n",
    "\n",
    "# # These will be used by AIF to compare the classifications of the model with the original values to\n",
    "# # understand if the model's answers create favouritism toward the privileged attribute\n",
    "\n",
    "\n",
    "# # This is the object made of the original dataset\n",
    "# aif_sex_dataset = BinaryLabelDataset(  # Base class for all structured datasets with binary labels.\n",
    "#     df=dataset,\n",
    "#     # This means that a prediction is biased toward the privileged attribute if its value is 1 (True)\n",
    "#     favorable_label=1,\n",
    "#     unfavorable_label=0,\n",
    "#     label_names=[target],\n",
    "#     protected_attribute_names=trump_features,\n",
    "#     # here we tell AIF that we want to check for predictions\n",
    "#     privileged_protected_attributes=['does_contain_trump'],\n",
    "#     # that somehow privilege the attribute \"sex_Male\"\n",
    "# )\n",
    "\n",
    "# # We do the same thing but with the predictions dataset\n",
    "# aif_sex_pred = BinaryLabelDataset(\n",
    "#     df=predictions,\n",
    "#     favorable_label=1,\n",
    "#     unfavorable_label=0,\n",
    "#     label_names=[target],\n",
    "#     protected_attribute_names=trump_features,\n",
    "#     privileged_protected_attributes=['does_contain_trump'],\n",
    "# )\n",
    "\n",
    "# trump_privileged_group = [{'does_contain_trump': 1}]\n",
    "# trump_unprivileged_group = [{'does_contain_trump': 0}]    \n",
    "\n",
    "# # We provide the ClassificationMetric object with all the information needed:\n",
    "# # aif_sex_dataset - The original test set\n",
    "# # aif_sex_pred - A dataset containing the predictions of the model\n",
    "# # sex_privileged_group - The privileged group\n",
    "# # sex_unprivileged_group - The unprivileged group\n",
    "# fairness_metrics = ClassificationMetric(dataset=aif_sex_dataset,\n",
    "#                                         classified_dataset=aif_sex_pred,\n",
    "#                                         unprivileged_groups=trump_unprivileged_group,\n",
    "#                                         privileged_groups=trump_unprivileged_group)\n",
    "\n",
    "# # Values less than 0 indicate that privileged group has higher\n",
    "# # proportion of predicted positive outcomes than unprivileged group.\n",
    "# # Value higher than 0 indicates that unprivileged group has higher proportion\n",
    "# # of predicted positive outcomes than privileged group.\n",
    "# SPD = round(fairness_metrics.statistical_parity_difference(), 3)\n",
    "\n",
    "# # Measures the deviation from the equality of opportunity, which means that the same\n",
    "# # proportion of each population receives the favorable outcome. This measure must be equal to 0 to be fair.\n",
    "# EOD = round(fairness_metrics.equal_opportunity_difference(), 3)\n",
    "\n",
    "# # Average of difference in False Positive Rate and True Positive Rate for unprivileged and privileged groups\n",
    "# # A value of 0 indicates equality of odds, which means that samples in both the privileged and unprivileged\n",
    "# # groups have the same probability of being classified positively.\n",
    "# AOD = round(fairness_metrics.average_odds_difference(), 3)\n",
    "\n",
    "# print(f\"Statistical Parity Difference (SPD): {SPD}\")\n",
    "# print(f\"Equal Opportunity Difference (EOD): {EOD}\")\n",
    "# print(f\"Average Odds Difference: {AOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import warnings\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# df = pd.read_csv('WELFake_Dataset.csv')\n",
    "\n",
    "# # bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "# # df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "# X = df.drop(columns=['label'])\n",
    "# y = df['label']\n",
    "\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# X = X_smote\n",
    "# y = y_smote\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "# from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0, '..')\n",
    "# from load_model import load_model, predict\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df = pd.read_csv('fake_and_real_data.csv')\n",
    "# df[\"y\"] = [1 if y == 'Real' else 0 for y in df['label']] \n",
    "# df[\"y_pred\"] = [\"Real\" for y in df['Text']]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_to_source = \"https://towardsdatascience.com/analysing-fairness-in-machine-learning-with-python-96a9ab0d0705\"\n",
    "# df = pd.read_csv('fake_and_real_data.csv')\n",
    "# model, tokenizer, device = load_model()\n",
    "\n",
    "# df_fair = df[['label']]\n",
    "# df[\"y\"] = [1 if y == 'Real' else 0 for y in df['y']] \n",
    "# df[\"y_pred\"] = [model.predict(y, model, tokenizer, device) for y in df['Text']]\n",
    "# counts = df['text'].value_counts()\n",
    "# labels = counts.index\n",
    "\n",
    "# #Plot pie chart\n",
    "# plt.pie(counts, startangle=90)\n",
    "# plt.legend(labels, loc=2,fontsize=15)\n",
    "# plt.title(\"Truthness of news\",size=20)\n",
    "# result = fairness_metrics(df)\n",
    "\n",
    "# def fairness_metrics(df):\n",
    "#     \"\"\"Calculate fairness for subgroup of population\"\"\"\n",
    "    \n",
    "#     #Confusion Matrix\n",
    "#     cm=confusion_matrix(df['y'],df['y_pred'])\n",
    "#     TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "#     N = TP+FP+FN+TN #Total population\n",
    "#     ACC = (TP+TN)/N #Accuracy\n",
    "#     TPR = TP/(TP+FN) # True positive rate\n",
    "#     FPR = FP/(FP+TN) # False positive rate\n",
    "#     FNR = FN/(TP+FN) # False negative rate\n",
    "#     PPP = (TP + FP)/N # % predicted as positive\n",
    "    \n",
    "#     return np.array([ACC, TPR, FPR, FNR, PPP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
